{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f093f4a-45de-461a-a1fd-5d74551f96d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22be91-8d8d-4b90-a523-ba3e4da40440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Window-example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5474e8d1-8ec0-433f-8c61-c67ab9191985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+\n",
      "|partition_col|order_col|value|\n",
      "+-------------+---------+-----+\n",
      "|            1|        A|   10|\n",
      "|            1|        B|   20|\n",
      "|            2|        A|   30|\n",
      "|            2|        B|   40|\n",
      "|            2|        C|   50|\n",
      "+-------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"A\", 10), (1, \"B\", 20), (2, \"A\", 30), (2, \"B\", 40), (2, \"C\", 50)]\n",
    "columns = [\"partition_col\", \"order_col\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd9e996-5243-4584-98bb-c50c696b11b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x104fbdd30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a window specification\n",
    "window_spec = (\n",
    "    Window\n",
    "    .partitionBy(\"partition_col\") # 각 파티션의 데이터를\n",
    "    .orderBy(\"order_col\") # order_col의 순서대로\n",
    ")\n",
    "window_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f24df2d-0355-4a26-bf15-b22a5e3e5c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------------+\n",
      "|partition_col|order_col|value|running_total|\n",
      "+-------------+---------+-----+-------------+\n",
      "|            1|        A|   10|           10|\n",
      "|            1|        B|   20|           30|\n",
      "|            2|        A|   30|           30|\n",
      "|            2|        B|   40|           70|\n",
      "|            2|        C|   50|          120|\n",
      "+-------------+---------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate running total within each partition\n",
    "result = df.withColumn(\"running_total\", F.sum(\"value\").over(window_spec))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515c81bd-d668-4f59-bd7d-2daf91358951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+\n",
      "|     name|dollars|       timestampGMT|\n",
      "+---------+-------+-------------------+\n",
      "|tshilidzi|   17.0|2018-03-10 16:27:18|\n",
      "|tshilidzi|   13.0|2018-03-11 13:27:18|\n",
      "|tshilidzi|   25.0|2018-03-12 12:27:18|\n",
      "|    thabo|   20.0|2018-03-13 16:27:18|\n",
      "|    thabo|   56.0|2018-03-14 13:27:18|\n",
      "|    thabo|   99.0|2018-03-15 12:27:18|\n",
      "|tshilidzi|  156.0|2019-03-22 12:27:18|\n",
      "|    thabo|  122.0|2018-03-31 13:27:18|\n",
      "|tshilidzi| 7000.0|2019-04-15 13:27:18|\n",
      "|      ash| 9999.0|2018-04-16 13:27:18|\n",
      "+---------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"tshilidzi\", 17.00, \"2018-03-10T15:27:18+00:00\"), \n",
    "  (\"tshilidzi\", 13.00, \"2018-03-11T12:27:18+00:00\"),   \n",
    "  (\"tshilidzi\", 25.00, \"2018-03-12T11:27:18+00:00\"), \n",
    "  (\"thabo\", 20.00, \"2018-03-13T15:27:18+00:00\"), \n",
    "  (\"thabo\", 56.00, \"2018-03-14T12:27:18+00:00\"), \n",
    "  (\"thabo\", 99.00, \"2018-03-15T11:27:18+00:00\"), \n",
    "  (\"tshilidzi\", 156.00, \"2019-03-22T11:27:18+00:00\"), \n",
    "  (\"thabo\", 122.00, \"2018-03-31T11:27:18+00:00\"), \n",
    "  (\"tshilidzi\", 7000.00, \"2019-04-15T11:27:18+00:00\"),\n",
    "  (\"ash\", 9999.00, \"2018-04-16T11:27:18+00:00\") \n",
    "  ],\n",
    "  [\"name\", \"dollars\", \"timestampGMT\"])\n",
    "\n",
    "# we need this timestampGMT as seconds for our Window time frame\n",
    "df = df.withColumn('timestampGMT', df.timestampGMT.cast('timestamp'))\n",
    "\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a75a1e54-1ab8-496e-8b64-dbc1f333050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "days = lambda i: i * 86400\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy(\"name\") \n",
    "    .orderBy(F.col(\"timestampGMT\").cast('long'))\n",
    "    .rangeBetween(-days(2), 0) # 이름별로, 그 row 포함 ~ 그 전 2일까지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "987ae980-2aae-4b0f-b67f-a607da12f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+------------+\n",
      "|     name|dollars|       timestampGMT|sum_two_days|\n",
      "+---------+-------+-------------------+------------+\n",
      "|      ash| 9999.0|2018-04-16 13:27:18|      9999.0|\n",
      "|    thabo|   20.0|2018-03-13 16:27:18|        20.0|\n",
      "|    thabo|   56.0|2018-03-14 13:27:18|        76.0|\n",
      "|    thabo|   99.0|2018-03-15 12:27:18|       175.0|\n",
      "|    thabo|  122.0|2018-03-31 13:27:18|       122.0|\n",
      "|tshilidzi|   17.0|2018-03-10 16:27:18|        17.0|\n",
      "|tshilidzi|   13.0|2018-03-11 13:27:18|        30.0|\n",
      "|tshilidzi|   25.0|2018-03-12 12:27:18|        55.0|\n",
      "|tshilidzi|  156.0|2019-03-22 12:27:18|       156.0|\n",
      "|tshilidzi| 7000.0|2019-04-15 13:27:18|      7000.0|\n",
      "+---------+-------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('sum_two_days', F.sum('dollars').over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408a2ed-b027-4dd3-921f-adb49da6cc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
